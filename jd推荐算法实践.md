# jd直播分享 



###  召回

如何构建用户向量表示， 基于向量召回

#### **微观用户画像**     

**背景：** 

 ![image-20201105205734559](https://i.loli.net/2020/11/05/uPlUWF1Z3mrTHNc.png)   

微观用户行为序列 - 更细致的用户行为分析 对具体某一商品的浏览记录更详细  是否点击评论 浏览时长等相关信息, 与商品交互过程中的 微观行为 

**方法：** 

![image-20201105210404748](https://i.loli.net/2020/11/05/jlei7Yb6XykuV3W.png)

#### 层次用户画像 

**背景** ：

- 兴趣是又层次的 先粗粒度后细粒度  （）

- 商品的分类也是有层次的 （这和一级二级分类有啥区别？ -- 只是一个单独的特征）

**方法**：

在**模型**中同样构建**层级结构**  

![image-20201105211635983](https://i.loli.net/2020/11/05/YItlzNHWsTi3Pm7.png)

- 不同点- **层次的rnn** 在一个网络中同时对不同层级学习   各个层有不同loss 
- **改进的RNN**  多了两个gate -- behavior gate 和time gate  



### 排序 

主要介绍了多任务学习和强化学习在jd的应用： 

1.  **多任务学习**  AMoe  在**MMoE**上的提升  加了attention机制 ，--每个任务学习一个查询向量，与专家网络最后一层计算得分，作为该任务对相应专家网络的权重。

2. 多任务学习 之 **多推荐场景融合** 联合优化 **迁移学习**   **联合模型**

   1. 多任务学习联合训练  二维的多任务学习【 推荐场景 * 优化目标】 

      之前的是一个场景下的 多个优化目标 现在增加多个场景 

3. **强化学习** ？  KDD-2019   

   1. Q网络（负责推荐）和S网络 （负责反馈）

   

   ![](https://i.loli.net/2020/11/05/MD3H2KlftJ6nVUp.png)

4. 基于强化学习的重排 

   强化学习  ： 动作和状态设计 -》 奖励函数设计 -》 策略优化算法 -》 在线结果   



### 展示  

![image-20201030003953566](https://i.loli.net/2020/10/30/ibjxYVTGA1w9FZE.png)

主要依靠**评论数据**  

![](https://i.loli.net/2020/11/05/nPzau3KmL8gxXoE.png)



前两个阶段主要是 粗排 看召回率 最后一个阶段要求精确率 （不能有bad case） 



在商品的问答部分-- 来挖掘推荐理由 

![image-20201030004512780](https://i.loli.net/2020/10/30/QlPhDxbGJ6tFjOr.png)



提到的一些论文

![image-20201030004620433](https://i.loli.net/2020/10/30/Z3DApn5Wqc8CltY.png)